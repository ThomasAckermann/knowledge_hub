Date: 05-05-2025
Tags: #InverseProblems #Statistics #MachineLearning 
## Inverse Problems
Many applications across sciences and technologies require determining causal factors from a set of observed effects. This process is called an inverse problem since it starts with the effects and then calculates the cause. As the name indicates, it is the inverse of a forward problem that starts with the causes to then compute effects.

The importance of inverse problems in applications, and hence in applied mathematics, is that they can tell us about parameters in mathematical models that we cannot observe directly. Furthermore the inverse problem framework can be extended beyond parameter identification problems to optimisation and control of design strategies of risk analysis.
To formulate inverse problems systematically, we start with a forward model, which depending on the field is often subject of research in its own right. As mentioned before a good forward model will describe how effects depend on causes. That is, the forward model is mathematical model $G : U \to Y$ that determines an output $y \in Y$ from some input $u \in U$, for suitable spaces $U$ and $Y$, typically Banach spaces. The mapping $G: U \to Y$ is sometomes also called observation operator. The corresponding inverse problem then reads:
"given $y \in Y$ find $u \in U$ such that $y = G(u)$"
## Well-posedness
A mathematical problem is called well-posed in the sense of Hadamard, if
1. the problem has a solution (existence)
2. the solution is unique (uniqueness)
3. the solution depends continuously on the problems inputs (stability)
If one of the conditions is not satisfied, then the problem is said to be ill posed.
Even if the forward problem is well posed, the inverse problem can be ill-posed. This is typical for inverse problems and mitigating the ill-posedness is and extensive part of the subject. The idea is replace the ill posed problem $u = G^{-1}(y)$ by a regularized well-posed version. There are different approaches to that. Here, we will use the bayesian framework, which offers and somewhat natural interpretation as we will see.
### Inverse Crimes
Sometimes numerical methods contain features that effectively render an inverse problems "less" ill-posed than the continuous actually is, thus yielding unrealistic optimistic results. This is sometimes coined inverse crimes and can be summarized by claiming that the model and the reality are correctly identified, in other words one believes that the computational model is exact. In practice inverse crimes arise for example
- synthetic numerically produced simulated data is generated by the same model that is used for inversion and
- discretization in the computational model is the same as the one used to invert the data


---
## References
[[Baye's Theorem]]