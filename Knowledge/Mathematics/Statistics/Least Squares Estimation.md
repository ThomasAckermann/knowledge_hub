#MachineLearning #Numerics #Statistics 

## Bayesian linear minimum mean square error (LMMSE) estimator

The bayesian linear minimum mean square error is given by $\hat{\theta} = \mathbb{E}(\theta) + C_{\theta x} C^{-1}_x [x- \mathbb{E}(x)]$. The mean square error (MSE) achieved by the bayesian LMMSE estimator is
$\text{BMSE}(\hat{\theta}) = C_\theta- C_{\theta x} C^{-1}_x C_{x\theta}$.
The LMMSE estimator works for generic data models. We make no assumptions on the relation between the data $x$ and the parameters $\theta$.
The LMMSE estimator is the best linear estimator (in the MSE sense). The MMSE estimator $\hat{\theta} = \mathbb{E}(\theta \mid x)$ might perform better but linear estimators are usually simpler and often good enough.
### Bayesian Gauss-Markov Theorem
If the data $x$ is generated by the bayesian linear model $x = H \theta + w$. with $\mathbb{E}(w)=0$ and $\mathbb{E}(\theta w^\top)=0$, then the Bayesian LMMSE estimator is $\hat{\theta} = \mathbb{E}(\theta) + C_\theta H^\top (H C_\theta H^\top + C_w)^{-1} H C_\theta$.
Finally, the achieved MSE is given by $\text{BMSE}(\hat{\theta}) = C_\epsilon$. 
### Sequential LMMSE
If the noise sequence $w[0], w[1], \ldots$ is uncorrelated, we can compute the Bayesian LMMSE estimator using the following sequential scheme.
Initialization:
- $\hat{\theta}[-1] = \mathbb{E}(\theta)$, $C_\epsilon [-1] = C_\theta$
Estimator Update:
- $\hat{\theta}[n] = \hat{\theta}[n-1] + K[n](x[n]- G[n]\hat{\theta}[n-1])$
- where $K[n] = C_\epsilon [n-1] G^\top [n] (C_{w[n]} + G[n] C_\epsilon [n-1]G^\top [n])^{-1}$.
Error covariance update:
- $C_\epsilon [n] = (\mathbb{1} - K[n]G[n])C_\epsilon [n-1]$ 

The computational complexity of updating the estimator using these formulas is independent of the number of data samples. For scalar data, no matrix inversion is required at all.
